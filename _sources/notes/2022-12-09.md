---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.14.1
kernelspec:
  display_name: Python 3
  name: python3
---

```{code-cell}
:id: xEsxJEFCRyyW

import numpy as np     # advanced math library
import matplotlib.pyplot as plt  
import random      # for generating random numbers

from keras.datasets import mnist # MNIST dataset is included in Keras  
from keras.models import Sequential # Model type to be used

from keras.layers.core import Dense, Dropout, Activation # Types of layers to be used in our model
from keras.utils import np_utils       # NumPy related tools

from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Flatten
from keras.layers import BatchNormalization
```

```{code-cell}
---
colab:
  base_uri: https://localhost:8080/
id: 4W_p902oSJL9
outputId: 35cd2888-8170-4e07-8208-31bfec8e0ff8
---
# The MNIST data is split between 60,000 28 x 28 pixel training images and 10,000 28 x 28 pixel images
(X_train, y_train), (X_test, y_test) = mnist.load_data()
```

```{code-cell}
---
colab:
  base_uri: https://localhost:8080/
  height: 657
id: 5jWDqu1jSrPU
outputId: b7bc8b9d-a80c-4c83-a8db-cb3ff85cac09
---
plt.rcParams['figure.figsize'] = (9,9) # Make the figures a bit bigger

for i in range(9):
 plt.subplot(3,3,i+1)
 num = random.randint(0, len(X_train))
 plt.imshow(X_train[num], cmap='gray', interpolation='none')
 plt.title("Class {}".format(y_train[num]))

plt.tight_layout()
```

```{code-cell}
---
colab:
  base_uri: https://localhost:8080/
id: 8SW8FtcwTCUX
outputId: 959ef424-3d46-4176-aa3d-1e6d347528e1
---
X_train = X_train.reshape(60000, 784) # reshape 60,000 28 x 28 matrices into 60,000 784-length vectors.
X_test = X_test.reshape(10000, 784) # reshape 10,000 28 x 28 matrices into 10,000 784-length vectors.

X_train = X_train.astype('float32') # change integers to 32-bit floating point numbers
X_test = X_test.astype('float32')

X_train /= 255      # normalize each value for each pixel for the entire vector for each input
X_test /= 255

print("Training matrix shape", X_train.shape)
print("Testing matrix shape", X_test.shape)
```

```{code-cell}
---
colab:
  base_uri: https://localhost:8080/
id: cuDJNByCTj2L
outputId: 397256e6-3b2e-422b-fbff-621bdad764c6
---
y_train[0]
```

```{code-cell}
:id: 9nxjixdyUFGN

nb_classes = 10 # number of unique digits

Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)
```

```{code-cell}
---
colab:
  base_uri: https://localhost:8080/
id: 8L7T1D3DUYaK
outputId: 0ea3431c-d7e6-40b1-da4f-caa7f6f9f9b5
---
Y_train[0]
```

```{code-cell}
:id: MdWOeCD3UoP4

model = Sequential()
```

```{code-cell}
:id: Fv1XzCzeVCFp

model.add(Dense(512, input_shape=(784,)))
model.add(Activation('relu'))
```

```{code-cell}
:id: cKiFcHrwVYHi

model.add(Dense(512))
model.add(Activation('relu'))
```

```{code-cell}
:id: wSrMd4owVeiJ

model.add(Dense(10))
model.add(Activation('softmax'))
```

```{code-cell}
---
colab:
  base_uri: https://localhost:8080/
id: 9d6I5hvpVx8X
outputId: f89f0f69-efdd-43f8-8030-6ace7b09fbd6
---
model.summary()
```

```{code-cell}
:id: nSFTg2W7V0jq

# Let's use the Adam optimizer for learning
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
```

```{code-cell}
---
colab:
  base_uri: https://localhost:8080/
id: 3q6ndoVEW4i-
outputId: 4b5b890b-955b-4eb6-965a-955ba4d3c4a1
---
model.fit(X_train, Y_train,
   batch_size=128, epochs=5,
   verbose=1)
```

```{code-cell}
---
colab:
  base_uri: https://localhost:8080/
id: V6tzrRHiXs_W
outputId: fed9feb8-d9d1-4701-8983-1176c05b02a8
---
score = model.evaluate(X_test, Y_test)
score
```

```{code-cell}
---
colab:
  base_uri: https://localhost:8080/
  height: 1000
id: 4caxG7TvYBFk
outputId: b565e409-d21b-4571-9f40-59a1258d2bfb
---
predicted_prob = model.predict(X_test)
predicted_classes = np.argmax(predicted_prob,axis=1)



# Check which items we got right / wrong
correct_indices = np.nonzero(predicted_classes == y_test)[0]

incorrect_indices = np.nonzero(predicted_classes != y_test)[0]


plt.figure()
for i, correct in enumerate(correct_indices[:9]):
 plt.subplot(3,3,i+1)
 plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')
 plt.title("Predicted {}, Class {}".format(predicted_classes[correct], y_test[correct]))

plt.tight_layout()

plt.figure()
for i, incorrect in enumerate(incorrect_indices[:9]):
 plt.subplot(3,3,i+1)
 plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')
 plt.title("Predicted {}, Class {}".format(predicted_classes[incorrect], y_test[incorrect]))

plt.tight_layout()
```

```{code-cell}
:id: ffpXFzspYW3R

model_dropout = Sequential()

model_dropout.add(Dense(512, input_shape=(784,)))
model_dropout.add(Activation('relu'))
model_dropout.add(Dropout(0.2))

model_dropout.add(Dense(512))
model_dropout.add(Activation('relu'))
model_dropout.add(Dropout(0.2))

model_dropout.add(Dense(10))
model_dropout.add(Activation('softmax'))
```

```{code-cell}
---
colab:
  base_uri: https://localhost:8080/
id: f3XymuKJZX0N
outputId: 77f327e1-37cc-4f50-d106-0f12ffc2a7f5
---
model_dropout.summary()
```

```{code-cell}
---
colab:
  base_uri: https://localhost:8080/
id: KhPfrep7ZaNH
outputId: 261e5c3f-41ff-4cbc-ff1e-3e4c4529c743
---
# Let's use the Adam optimizer for learning
model_dropout.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_dropout.fit(X_train, Y_train,
   batch_size=128, epochs=5,
   verbose=1)
```

```{code-cell}
---
colab:
  base_uri: https://localhost:8080/
id: 46Wmv4EZZeUU
outputId: 77042f81-b4cc-4a89-eb04-b25cfb26b582
---
score = model_dropout.evaluate(X_test, Y_test)
score
```

```{code-cell}
:id: wECPQFRbZh4r


```

+++ {"id": "ir5wX5xTAR4F"}

### can this only be applied in google?


### Are there any general rules for tuning parameters, such as a typical percentage of values to dropout

### Will we ever use Jupyter Notebook versus a different code compiler / environment in real jobs?
